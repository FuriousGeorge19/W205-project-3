{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joe Mirza - Project 3 - W205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "### **My project has the following files:**  \n",
    "**1. Project_3_Joe_Mirza_w205.ipynb:** This notebook. To narrate the pipeline steps and perform a few presto queries once the data is landed in hadoop.   <br>  \n",
    "**2. ab.sh:** A bash script I run from within a while loop that runs on the command line and streams events into the pipeline using apache bench.   <br>  \n",
    "**3. game_api.py:** Takes the events from the last step and routes them from flask into kafka. Maps those incoming events into event dictionaries, appends header information and packages each bundle into a json before sending the object into kafka.    <br>  \n",
    "**4. stream_and_hive.py:** Reads the event objects from kafka. Uses pyspark to filter the 4 events types I'm supporting (default, purchase_sword, purchase_armor and join_a_guild) into 4 separate tables which are registered in hive and written to hadoop.     <br>  \n",
    "**5. docker-compose.yml:** Spins up the cluster, maps/coordinates the interaction between some of the services. Will pull a few pieces out of that file into this one inline in this notebook to demonstrate I understand how they work.   <br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will spin up the cluster with:        `     docker-compose up -d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Services spun up:**\n",
    "- zookeeper\n",
    "- kafka\n",
    "- cloudera\n",
    "- spark\n",
    "- presto\n",
    "- mids\n",
    "<br>  \n",
    "The only new service here, relative to what we've done in Project 1 and 2, is presto. Spark is used in this project to filter, transform and ultimately write events to hadoop. We'll use presto in this notebook to perform sql queries.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start up kafkacat to 'consume' or receive events from apache bench:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed the approach laid out in unit 13a, which didn't create the topic 'events' first but rather relied on the fact that kafkacat will create a topic 'events' if it doesn't already exist. But that requires entering the above command twice. Seems odd, but it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the above command and the .yml, kafka is listening on port 29092\n",
    "    ```\n",
    "      kafka:\n",
    "        image: confluentinc/cp-kafka:latest\n",
    "        depends_on:\n",
    "          - zookeeper\n",
    "        environment:\n",
    "          KAFKA_BROKER_ID: 1\n",
    "          KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "        expose:\n",
    "          - \"9092\"\n",
    "          - \"29092\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Before running flask, we'll discuss what game_api.py does and how I modified it. I'll only show the method and flask decorator I created so you don't have to wade through the whole thing. If you need to see all of game_api.py, it's in the repository.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.flask is imported to read and, well, 'route' the apache bench messages it receives  <br>    \n",
    "2.KafkaProducer is imported to send the events that are transformed here into the kafka queue   <br>  \n",
    "3.request is imported to append header information to the transformed events    <br>  \n",
    "4.Each of the 4 events type I'm supporting has a different flask decorator and associated python method. The one I created is join_a_guild. When flask's @app.route 'sees' an incoming event from apache bench ending in \"/join_a_guild\", it creates a join_guiild_event dictionary and sends it to the log_to_kakfa method. There the header is appended and the whole thing is sent to the `events` topic as a json by kafka producer. <br>  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ```\n",
    "    #!/usr/bin/env python\n",
    "    import json\n",
    "    from kafka import KafkaProducer\n",
    "    from flask import Flask, request\n",
    "\n",
    "    app = Flask(__name__)\n",
    "    producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "    def log_to_kafka(topic, event):\n",
    "        event.update(request.headers)\n",
    "        producer.send(topic, json.dumps(event).encode())\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "    @app.route(\"/join_a_guild\")\n",
    "    def join_a_guild():\n",
    "        join_guild_event = {'event_type': 'join_a_guild'}\n",
    "        log_to_kafka('events', join_guild_event)\n",
    "        return \"Guild Joined!\\n\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let's run flask now**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `docker-compose exec mids env FLASK_APP=/w205/project-3-FuriousGeorge19/game_api.py flask run --host 0.0.0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **With flask and kafka running, now is a good point to test whether what we've built up until this point is working by using apache bench to stream some events and see if they're picked up by flask and kafka **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use the while loop in the live session slides, that had the apache bench calls in them, I thought I'd try to use the while loop to run the bash script ab.sh repeatedly instead. I was pleasantly surprised when it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      " \r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_armor\r\n",
      "\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_armor\r\n",
      "\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_armor\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ab.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified loop sends lots and lots (I guess 120 events) each time the loop runs. \n",
    "\n",
    "    ```\n",
    "    \n",
    "    while true; do \n",
    "      ./ab.sh; \n",
    "      sleep 10;  \n",
    "    done\n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project_3_Joe_Mirza_w205.ipynb\tdocker-compose.yml  stream_and_hive.py\r\n",
      "README.md\t\t\tgame_api.py\r\n",
      "ab.sh\t\t\t\tgame_api.pyc\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      " \r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_armor\r\n",
      "\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_armor\r\n",
      "\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild\r\n",
      "docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_armor\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ab.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
